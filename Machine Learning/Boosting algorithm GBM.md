# Boosting algorithm: GBM

This article continues the previous post [Boosting algorithm:
AdaBoost](https://medium.com/@SauceCat/boosting-algorithm-adaboost-b6737a9ee60c).
This time we will turn to GBM (Gradient Boosting Machine).

## Tree models

Before diving into tree boosting algorithms, let‚Äôs first review the learning
procedure of tree models. Tree models partition the feature space **X** into a
set of **T** non-overlapping rectangular regions. For each region **R1, ‚Ä¶, RT**,
prediction is usually generated by a simple constant model.

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*Um8Ng1RO9Y8JWuK47_b_8Q.jpeg'></p>

There are various algorithms for learning tree models, like **CART, C4.5** and
**CHAID**. For **GBM, CART** is used and **XGBoost** also utilizes an algorithm
similar to **CART**. Thus, we will only discuss **CART** in this post.

**CART** grows the tree greedily in a top-down fashion using binary splits. For
each tree node, every split parallel to the coordinate axes are considered and
the one minimizing the objective is chosen. The split procedure is repeated
until some stopping criterion is reached.

### Learning the tree structure

The loss function for a tree **f** with **T** terminal nodes can be written as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*Kf5ekcPKBCEDV2V_YJ4dRw.png'></p>

where **L_j** is the aggregated loss at node **j**. Consider we are going to
split at node **k**, the split procedure is represented as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*koIQu8cFCeunMg2S6cZUhg.png'></p>

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*UeO2jzXERjDPNDJ7Yhwesg.png'></p>

The gain of split is defined as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*zWvZRHvJ3qELhGtaEZ28Iw.png'></p>

For each split made, the split gain is calculated for every possible split for
every possible feature and the one with the maximal gain is taken.

### Learning the weights for a given structure

Given the learned structure, for a region **R_j**, the weight is estimated by

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*3wgCoQEGQBLd237gNS16MA.jpeg'></p>

For example, for squared error loss, the estimated weight will simply be the
average of all responses in the region.

## Boosting algorithm

Boosting algorithm fits the ensemble models of the kind

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*ERegahgd879qqEBxo8Vleg.jpeg'></p>

where f‚Äã0 is the initial guess, **œïm(x)** is the base estimator at iteration
**m** and **Œ∏‚Äãm** is the weight for the **m_th** estimator. The product
**Œ∏‚Äãm*œïm(x)** denotes the ‚Äústep‚Äù at iteration **m**.

Most boosting algorithms can be viewed as to solve

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*2geEAAEM-_8hChyfr-sPig.png'></p>

at each iteration, where **f(m-1)** denotes the current estimation. Therefore,
the ensemble problem is simplified greedily as a forward stage-wise additive
model. We don‚Äôt optimize the ensemble in a global manner, but instead seek to
improve the result based on the current estimate.

For AdaBoost, it solves this equation for the exponential loss function under
the constraint that **œïm(x)** only outputs -1 or 1. While **GBM** and
**XGBoost** can be viewed as two general boosting algorithms that solve the
equation approximately for any suitable loss function.

## GBM: Gradient Boosting Machine

**GBM**, short for ‚ÄúGradient Boosting Machine‚Äù, is introduced by Friedman in
2001. It is also known as **MART** (Multiple Additive Regression Trees) and
**GBRT** (Gradient Boosted Regression Trees).

**GBM** constructs a forward stage-wise additive model by implementing gradient
descent in function space. Similar to gradient descent in parameter space, at
the **m_th** iteration, the direction of the steepest descent is given by the
negative gradient of the loss function:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*EUIz2s4aEqOR9a0pirNiiQ.png'></p>

Taking a step along this direction is guaranteed to reduce the loss.

At each iteration, a regression tree model is fitted to predict the negative
gradient. Typically the squared error is used as a surrogate loss.

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*V4sPR7IBDko4N5k82vptqg.png'></p>

However, the negative gradient only gives the direction of the step. Further
effort is necessary to determine the step length **œÅ‚Äãm**. A popular way is to do
line search:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*coK0thlkc5DRN7SANLUnzg.png'></p>

Regarding this, Friedman presented a special enhancement. Considering that at
each iteration, the algorithm was actually generating **T** separate predictions
for each region **R1, ‚Ä¶, RT**, he proposed to do** T** line search steps, one
for each region.

Friedman also introduced shrinkage, where the step length at each iteration is
multiplied by some factor **Œ∑ (0<Œ∑‚â§1)**, which is proved to enhance the model
performance in practice. **Œ∑** is also referred as the learning rate, as
lowering it will slow down the learning process. Combining all of these, the
‚Äústep‚Äù taken at each iteration **m** is give by

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*c9c87u030xL_KNuvQ4_S4g.png'></p>

where the step length **œÅ‚Äãm** can be different for each region.

Then the resulting model can be written as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*yM0HrpywOCrcp1xWlWAhtA.png'></p>

where **f0** is typically initialized using a constant:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*MfWZhIiubaTy0jf7UB4Mjg.png'></p>

With big picture in mind, let‚Äôs now go deeper into the base estimator. First,
the loss function can be rewritten as follows:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*BNyPBswsvI17CmF4sO0urA.png'></p>

where **n_jm** is number of instances rest in region** j**. Letting **G_jm**
represents the sum of gradient in region **j**, the equation can be further
rewritten as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*7hQkk97PxMo3jrsQduywNg.png'></p>

For a fixed learned structure, we can directly calculate the weights that
minimize the loss function:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*qHGCjkjO8WosaPu5MT6_lA.png'></p>

Plugging the weight back to the loss function, the loss function becomes:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*_rhsIO6YhHCSAWtn3EPU7A.png'></p>

Then, for a single node **j**, the proxy gain (ignore the other nodes and the
normalization term) of a split is defined as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*fjAhxKQn1udxCHy5HcAaOw.png'></p>

where **G_jmL** denotes the left node of the split and **G_jmR** denotes the
right node.

## Walk through sklearn GBM source code üíª

For better understanding, we are going to walk through the source code of
[sklearn.ensemble.GradientBoostingClassifier](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/ensemble/gradient_boosting.py#L1265).
For simplification, we will only focus on binary classification and the most
important code snippets.

#### class GradientBoostingClassifier
```python
class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
    _SUPPORTED_LOSS = ('deviance', 'exponential')

    def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,
                 subsample=1.0, criterion='friedman_mse', min_samples_split=2,
                 min_samples_leaf=1, min_weight_fraction_leaf=0.,
                 max_depth=3, min_impurity_split=1e-7, init=None,
                 random_state=None, max_features=None, verbose=0,
                 max_leaf_nodes=None, warm_start=False,
                 presort='auto'):
```

The default criterion for GBM is `friedman_mse`, a modified kind of `mse` specially for boosting algorithms. However, in our case, let‚Äôs just stick to the simpliest `mse`. In terms of loss function, `deviance` is default for binary classification problem, while `exponential` is specially for AdaBoost.

#### fit function inherited from BaseGradientBoosting
```python
def fit(self, X, y, sample_weight=None, monitor=None):
    if not self._is_initialized():
        # init state
        self._init_state()
        # fit initial model - FIXME make sample_weight optional
        self.init_.fit(X, y, sample_weight)
        # init predictions
        y_pred = self.init_.predict(X)
        begin_at_stage = 0
    # fit the boosting stages
    n_stages = self._fit_stages(X, y, y_pred, sample_weight, random_state, begin_at_stage, monitor, X_idx_sorted)
return self
```
The fit function is inherited from `class BaseGradientBoosting`. Firstly, the learning process is initialized by `self._init_state()`, where loss class as well as `init_estimator` is assigned.

#### learning process initialization
```python
def _init_state(self):
    if self.init is None:
        self.init_ = self.loss_.init_estimator()
    self.estimators_ = np.empty((self.n_estimators, self.loss_.K), dtype=np.object)
    self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
    
# determine the loss function
def _check_params(self):
    if self.loss == 'deviance':
        # our loss class is BinomialDeviance
        loss_class = (MultinomialDeviance if len(self.classes_) > 2 else BinomialDeviance)

    if self.loss in ('huber', 'quantile'):
        self.loss_ = loss_class(self.n_classes_, self.alpha)
    else:
        self.loss_ = loss_class(self.n_classes_)

# find the init_estimator for BinomialDeviance loss class
class BinomialDeviance(ClassificationLossFunction):
    def init_estimator(self):
        # the init_estimator is LogOddsEstimator
        return LogOddsEstimator()
```
The initial prediction is generated through `self.init_.fit()` and `self.init_.predict()`, where `init_estimator` is assigned as `LogOddsEstimator`.

#### initial prediction
```python
# init_estimator
class LogOddsEstimator(BaseEstimator):
    scale = 1.0
    def fit(self, X, y, sample_weight=None):
        # pre-cond: pos, neg are encoded as 1, 0
        if sample_weight is None:
            pos = np.sum(y)
            neg = y.shape[0] - pos
        else:
            pos = np.sum(sample_weight * y)
            neg = np.sum(sample_weight * (1 - y))
            
        self.prior = self.scale * np.log(pos / neg)

    def predict(self, X):
        check_is_fitted(self, 'prior')
        y = np.empty((X.shape[0], 1), dtype=np.float64)
        # fill y with initial prediction
        y.fill(self.prior)
        return y
```
After initialization, `self._fit_stages` is started to fit the boosting stages.
```python
def _fit_stages(self, X, y, y_pred, sample_weight, random_state, begin_at_stage=0, monitor=None, X_idx_sorted=None):
    loss_ = self.loss_
    # perform boosting iterations
    i = begin_at_stage
    
    for i in range(begin_at_stage, self.n_estimators):
        # fit next stage of trees
        y_pred = self._fit_stage(i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)
        # track deviance (= loss)
        self.train_score_[i] = loss_(y, y_pred, sample_weight)
        
    return i + 1
```
For each stage, a `DecisionTreeRegressor` is fitted to predict the residual (negative gradient).
```python
def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc=None, X_csr=None):
    loss = self.loss_
    original_y = y
    
    # in our case, loss.K equals to 1
    for k in range(loss.K):
        if loss.is_multi_class:
            y = np.array(original_y == k, dtype=np.float64)
        # get the negative gradient 
        residual = loss.negative_gradient(y, y_pred, k=k, sample_weight=sample_weight)

        # induce regression tree on residuals
        tree = DecisionTreeRegressor(criterion=self.criterion, splitter='best', max_depth=self.max_depth, 
                                     min_samples_split=self.min_samples_split, min_samples_leaf=self.min_samples_leaf,
                                     min_weight_fraction_leaf=self.min_weight_fraction_leaf, max_features=self.max_features,
                                     max_leaf_nodes=self.max_leaf_nodes, random_state=random_state, presort=self.presort)
        
        tree.fit(X, residual, sample_weight=sample_weight, check_input=False, X_idx_sorted=X_idx_sorted)
        # update the terminal regions = line search
        loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred, sample_weight, sample_mask, self.learning_rate, k=k)
        # add tree to ensemble
        self.estimators_[i, k] = tree

    return y_pred
```
In details, `loss.negative_gradient()` and `loss.update_terminal_regions` is defined as follow.
```python
class BinomialDeviance(ClassificationLossFunction):
    def negative_gradient(self, y, pred, **kargs):
        return y - expit(pred.ravel())

    def _update_terminal_region(self, tree, terminal_regions, leaf, X, y, residual, pred, sample_weight):
        """Make a single Newton-Raphson step. Our node estimate is given by:
            sum(w * (y - prob)) / sum(w * prob * (1 - prob))
        we take advantage that: y - prob = residual
        """
        terminal_region = np.where(terminal_regions == leaf)[0]
        residual = residual.take(terminal_region, axis=0)
        y = y.take(terminal_region, axis=0)
        sample_weight = sample_weight.take(terminal_region, axis=0)

        numerator = np.sum(sample_weight * residual)
        denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))

        if denominator == 0.0:
            tree.value[leaf, 0, 0] = 0.0
        else:
            tree.value[leaf, 0, 0] = numerator / denominator
```
Remember we have mentioned the term `proxy gain`, which is defined as:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*fjAhxKQn1udxCHy5HcAaOw.png'></p>

In practice, this trick can be further simplified as:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*uUcXQO6cUP1rK6dHBp0hqA.png'></p>

It is utilized during the node split procedure of the regression tree.
```python
cdef class MSE(RegressionCriterion):
    cdef double proxy_impurity_improvement(self) nogil:
        """Compute a proxy of the impurity reduction
        This method is used to speed up the search for the best split.
        It is a proxy quantity such that the split that maximizes this value
        also maximizes the impurity improvement. It neglects all constant terms
        of the impurity decrease for a given split.
        The absolute impurity improvement is only computed by the
        impurity_improvement method once the best split has been found.
        """
        # sum_left is the sum gradient from left node
        cdef double* sum_left = self.sum_left
        # sum_right is the sum gradient from right node
        cdef double* sum_right = self.sum_right

        cdef SIZE_t k
        cdef double proxy_impurity_left = 0.0
        cdef double proxy_impurity_right = 0.0

        for k in range(self.n_outputs):
            proxy_impurity_left += sum_left[k] * sum_left[k]
            proxy_impurity_right += sum_right[k] * sum_right[k]

        return (proxy_impurity_left / self.weighted_n_left +
                proxy_impurity_right / self.weighted_n_right)
```

The true gain of split is only calcualted for the best split when it is found.
Note that the impurity improvement is scaled by the fraction of samples in that
node.
```python
cdef class Criterion:
    cdef double impurity_improvement(self, double impurity) nogil:
        """Compute the improvement in impurity
        This method computes the improvement in impurity when a split occurs.
        The weighted impurity improvement equation is the following:
            N_t / N * (impurity - N_t_R / N_t * right_impurity
                                - N_t_L / N_t * left_impurity)
        where N is the total number of samples, N_t is the number of samples
        at the current node, N_t_L is the number of samples in the left child,
        and N_t_R is the number of samples in the right child,
        Parameters
        ----------
        impurity : double
            The initial impurity of the node before the split
        Return
        ------
        double : improvement in impurity after the split occurs
        """

        cdef double impurity_left
        cdef double impurity_right

        self.children_impurity(&impurity_left, &impurity_right)

        return ((self.weighted_n_node_samples / self.weighted_n_samples) *
                (impurity - (self.weighted_n_right / 
                             self.weighted_n_node_samples * impurity_right)
                          - (self.weighted_n_left / 
                             self.weighted_n_node_samples * impurity_left)))
```

The true absolute node impurity is calculated as:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*QKIzHOubfTHePKSc165tCA.png'></p>

where m is the number of samples at that node.
```python
cdef class MSE(RegressionCriterion):
    """Mean squared error impurity criterion.
        MSE = var_left + var_right
    """

    cdef double node_impurity(self) nogil:
        """Evaluate the impurity of the current node, i.e. the impurity of
           samples[start:end]."""

        cdef double* sum_total = self.sum_total
        cdef double impurity
        cdef SIZE_t k

        impurity = self.sq_sum_total / self.weighted_n_node_samples
        for k in range(self.n_outputs):
            impurity -= (sum_total[k] / self.weighted_n_node_samples)**2.0

        return impurity / self.n_outputs
```

For prediction, probabilities are generated by `self.predict_proba()`. Initial prediction is given by `self._init_decision_function()`. At each stage, scores are updated by `predict_stages`. Finally, the scores are normalized as probabilities.
```python
class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
    def predict_proba(self, X):
        score = self.decision_function(X)
        try:
            return self.loss_._score_to_proba(score)
        except NotFittedError:
            raise
        except AttributeError:
            raise AttributeError('loss=%r does not support predict_proba' % self.loss)
            
    # self.decision_function
    def decision_function(self, X):
        score = self._decision_function(X)
        return score
      
    # self._decision_function
    # inherited from BaseGradientBoosting
    def _decision_function(self, X):
        score = self._init_decision_function(X)
        predict_stages(self.estimators_, X, self.learning_rate, score)
        return score
    
    # self._init_decision_function
    # inherited from BaseGradientBoosting
    def _init_decision_function(self, X):
        self._check_initialized()
        # first get the initial prediction
        score = self.init_.predict(X).astype(np.float64)
        return score
```
```python
class BinomialDeviance(ClassificationLossFunction):
    def _score_to_proba(self, score):
        proba = np.ones((score.shape[0], 2), dtype=np.float64)
        proba[:, 1] = expit(score.ravel())
        proba[:, 0] -= proba[:, 1]
        return proba
```
Where `expit` is the implementation of logistic sigmoid function.
```python
def expit(x, out=None):
    """Logistic sigmoid function, ``1 / (1 + exp(-x))``.
    See sklearn.utils.extmath.log_logistic for the log of this function.
    """
    if out is None:
        out = np.empty(np.atleast_1d(x).shape, dtype=np.float64)
    out[:] = x

    # 1 / (1 + exp(-x)) = (1 + tanh(x / 2)) / 2
    # This way of computing the logistic is both fast and stable.
    out *= .5
    np.tanh(out, out)
    out += 1
    out *= .5
    return out.reshape(np.shape(x))
```

Really feel so good to walk through the code! üòªüíØ
