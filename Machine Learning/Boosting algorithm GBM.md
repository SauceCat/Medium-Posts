# Boosting algorithm: GBM

This article continues the previous post [Boosting algorithm:
AdaBoost](https://medium.com/@SauceCat/boosting-algorithm-adaboost-b6737a9ee60c).
This time we will turn to GBM (Gradient Boosting Machine).

## Tree models

Before diving into tree boosting algorithms, let‚Äôs first review the learning
procedure of tree models. Tree models partition the feature space **X** into a
set of **T** non-overlapping rectangular regions. For each region **R1, ‚Ä¶, RT**,
prediction is usually generated by a simple constant model.

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*Um8Ng1RO9Y8JWuK47_b_8Q.jpeg'></p>

There are various algorithms for learning tree models, like **CART, C4.5** and
**CHAID**. For **GBM, CART** is used and **XGBoost** also utilizes an algorithm
similar to **CART**. Thus, we will only discuss **CART** in this post.

**CART** grows the tree greedily in a top-down fashion using binary splits. For
each tree node, every split parallel to the coordinate axes are considered and
the one minimizing the objective is chosen. The split procedure is repeated
until some stopping criterion is reached.

### Learning the tree structure

The loss function for a tree **f** with **T** terminal nodes can be written as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*Kf5ekcPKBCEDV2V_YJ4dRw.png'></p>

where **L_j** is the aggregated loss at node **j**. Consider we are going to
split at node **k**, the split procedure is represented as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*koIQu8cFCeunMg2S6cZUhg.png'></p>

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*UeO2jzXERjDPNDJ7Yhwesg.png'></p>

The gain of split is defined as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*zWvZRHvJ3qELhGtaEZ28Iw.png'></p>

For each split made, the split gain is calculated for every possible split for
every possible feature and the one with the maximal gain is taken.

### Learning the weights for a given structure

Given the learned structure, for a region **R_j**, the weight is estimated by

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*3wgCoQEGQBLd237gNS16MA.jpeg'></p>

For example, for squared error loss, the estimated weight will simply be the
average of all responses in the region.

## Boosting algorithm

Boosting algorithm fits the ensemble models of the kind

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*ERegahgd879qqEBxo8Vleg.jpeg'></p>

where f‚Äã0 is the initial guess, **œïm(x)** is the base estimator at iteration
**m** and **Œ∏‚Äãm** is the weight for the **m_th** estimator. The product
**Œ∏‚Äãm*œïm(x)** denotes the ‚Äústep‚Äù at iteration **m**.

Most boosting algorithms can be viewed as to solve

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*2geEAAEM-_8hChyfr-sPig.png'></p>

at each iteration, where **f(m-1)** denotes the current estimation. Therefore,
the ensemble problem is simplified greedily as a forward stage-wise additive
model. We don‚Äôt optimize the ensemble in a global manner, but instead seek to
improve the result based on the current estimate.

For AdaBoost, it solves this equation for the exponential loss function under
the constraint that **œïm(x)** only outputs -1 or 1. While **GBM** and
**XGBoost** can be viewed as two general boosting algorithms that solve the
equation approximately for any suitable loss function.

## GBM: Gradient Boosting Machine

**GBM**, short for ‚ÄúGradient Boosting Machine‚Äù, is introduced by Friedman in
2001. It is also known as **MART** (Multiple Additive Regression Trees) and
**GBRT** (Gradient Boosted Regression Trees).

**GBM** constructs a forward stage-wise additive model by implementing gradient
descent in function space. Similar to gradient descent in parameter space, at
the **m_th** iteration, the direction of the steepest descent is given by the
negative gradient of the loss function:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*EUIz2s4aEqOR9a0pirNiiQ.png'></p>

Taking a step along this direction is guaranteed to reduce the loss.

At each iteration, a regression tree model is fitted to predict the negative
gradient. Typically the squared error is used as a surrogate loss.

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*V4sPR7IBDko4N5k82vptqg.png'></p>

However, the negative gradient only gives the direction of the step. Further
effort is necessary to determine the step length **œÅ‚Äãm**. A popular way is to do
line search:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*coK0thlkc5DRN7SANLUnzg.png'></p>

Regarding this, Friedman presented a special enhancement. Considering that at
each iteration, the algorithm was actually generating **T** separate predictions
for each region **R1, ‚Ä¶, RT**, he proposed to do** T** line search steps, one
for each region.

Friedman also introduced shrinkage, where the step length at each iteration is
multiplied by some factor **Œ∑ (0<Œ∑‚â§1)**, which is proved to enhance the model
performance in practice. **Œ∑** is also referred as the learning rate, as
lowering it will slow down the learning process. Combining all of these, the
‚Äústep‚Äù taken at each iteration **m** is give by

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*c9c87u030xL_KNuvQ4_S4g.png'></p>

where the step length **œÅ‚Äãm** can be different for each region.

Then the resulting model can be written as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*yM0HrpywOCrcp1xWlWAhtA.png'></p>

where **f0** is typically initialized using a constant:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*MfWZhIiubaTy0jf7UB4Mjg.png'></p>

With big picture in mind, let‚Äôs now go deeper into the base estimator. First,
the loss function can be rewritten as follows:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*BNyPBswsvI17CmF4sO0urA.png'></p>

where **n_jm** is number of instances rest in region** j**. Letting **G_jm**
represents the sum of gradient in region **j**, the equation can be further
rewritten as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*7hQkk97PxMo3jrsQduywNg.png'></p>

For a fixed learned structure, we can directly calculate the weights that
minimize the loss function:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*qHGCjkjO8WosaPu5MT6_lA.png'></p>

Plugging the weight back to the loss function, the loss function becomes:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*_rhsIO6YhHCSAWtn3EPU7A.png'></p>

Then, for a single node **j**, the proxy gain (ignore the other nodes and the
normalization term) of a split is defined as

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*fjAhxKQn1udxCHy5HcAaOw.png'></p>

where **G_jmL** denotes the left node of the split and **G_jmR** denotes the
right node.

## Walk through sklearn GBM source code üíª

For better understanding, we are going to walk through the source code of
[sklearn.ensemble.GradientBoostingClassifier](https://github.com/scikit-learn/scikit-learn/blob/14031f6/sklearn/ensemble/gradient_boosting.py#L1265).
For simplification, we will only focus on binary classification and the most
important code snippets.

#### class GradientBoostingClassifier

The default criterion for GBM is , a modified kind of  specially for boosting
algorithms. However, in our case, let‚Äôs just stick to the simpliest . In terms
of loss function,  is default for binary classification problem, while  is
specially for AdaBoost.

#### fit function inherited from BaseGradientBoosting

The fit function is inherited from . Firstly, the learning process is
initialized by , where  as well as  is assigned.

#### learning process initialization

The initial prediction is generated through  and , where  is assigned as .

#### initial prediction

After initialization,  is started to fit the boosting stages.

For each stage, a is fitted to predict the residual (negative gradient).

In details,  and  is defined as follow.

Remember we have mentioned the term , which is defined as:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*fjAhxKQn1udxCHy5HcAaOw.png'></p>

In practice, this trick can be further simplified as:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*uUcXQO6cUP1rK6dHBp0hqA.png'></p>

It is utilized during the node split procedure of the regression tree.

The true gain of split is only calcualted for the best split when it is found.
Note that the impurity improvement is scaled by the fraction of samples in that
node.

The true absolute node impurity is calculated as:

<p align='center'><img src='https://cdn-images-1.medium.com/max/800/1*QKIzHOubfTHePKSc165tCA.png'></p>

where m is the number of samples at that node.

For prediction, probabilities are generated by . Initial prediction is given by
. At each stage, scores are updated by . Finally, the scores are normalized as
probabilities.

Where  is the implementation of logistic sigmoid function.

Really feel so good to walk through the code! üòªüíØ
